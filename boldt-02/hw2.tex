\documentclass[a4paper]{article}
\usepackage[letterpaper, margin=1in]{geometry} % page format
\usepackage{listings} % this package is for including code
\usepackage{graphicx} % this package is for including figures
\usepackage{amsmath}  % this package is for math and matrices
\usepackage{amsfonts} % this package is for math fonts
\usepackage{tikz} % for drawings
\usepackage{hyperref} % for urls
\usepackage{enumitem} % for fancy enumerates
\usepackage{pgfplots}

\title{Deep Learning - Homework 2}
\author{Brendon Boldt}
\date{Oct/23/2017}

\begin{document}
\lstset{
    language=Python,
    frame=single,
    breaklines=true,
    postbreak=\mbox{\textcolor{gray}{$\hookrightarrow$}\space} 
}

\maketitle

%\lstinputlisting[language=Python,frame=single]{hw0.libraries.py}
\iffalse
\begin{lstlisting}
> python3 hw0.libraries.py
3.5.2 (default, Nov 17 2016, 17:05:23) 
[GCC 5.4.0 20160609]
1.13.1
0.19.1
0.19.0
2.0.2
0.20.3
1.3.0
\end{lstlisting}
\fi

% You will discuss your results and interpret them; what is this telling you about the training of an LSTM with respect to the number of neurons or learning rate or both? What can you learn from this?

% It would be really neat if you could produce a plot/graphic from the table where multiple every line represents a column in the table. Something like this.

\section{Results}

To test the hyperparameters of the model, we varied both base learning
rate and the number of hidden units. The code used to run this
experiment can be found in \texttt{hw2.py}.

\begin{table}[h]
 \caption{The table displays the accuracies resulting from the corresponding hidden layer size and learning rate. Note that ``LR'' stands for ``Learning Rate''.}
 \label{table}
 \begin{center}
  \begin{tabular}{c|cccccc}
    \hline \hline
    Neurons/LR &
        $1\cdot10^{-5}$ &
        $1\cdot10^{-4}$ &
        $1\cdot10^{-3}$ &
        $1\cdot10^{-2}$ &
        $1\cdot10^{-1}$ &
        $1\cdot10^{0}$ \\
    \hline
    $16$   & $0.0957$ & $0.1574$ & $0.3724$ & $0.7197$ & $0.6781$ & $0.3333$ \\
    $32$   & $0.0684$ & $0.1577$ & $0.4410$ & $0.8540$ & $0.7126$ & $0.1032$ \\
    $64$   & $0.1577$ & $0.2990$ & $0.7568$ & $0.8769$ & $0.4855$ & $0.0974$ \\
    $128$  & $0.1626$ & $0.4422$ & $0.8126$ & $0.8799$ & $0.0987$ & $0.1032$ \\
    $256$  & $0.2174$ & $0.6067$ & $0.8362$ & $0.8725$ & $0.1009$ & $0.0980$ \\
    $512$  & $0.2867$ & $0.7034$ & $0.8654$ & $0.6754$ & $0.0974$ & $0.1032$ \\
    $1024$ & $0.4777$ & $0.7645$ & $0.8640$ & $0.0892$ & $0.0980$ & $0.1028$ \\
    $2048$ & $0.5950$ & $0.8063$ & $0.8056$ & $0.1135$ & $0.1009$ & $0.0892$ \\
    $4096$ & $0.6625$ & $0.8105$ & $0.7913$ & $0.1009$ & $0.0958$ & $0.1009$ \\
    \hline \hline
  \end{tabular}
 \end{center}
\end{table}

In order to better visualize the data, we have included a graph
(Figure \ref{figure}) of accuracy vs. hidden unit count.

\begin{figure}[h]
\label{figure}
\centering
\begin{tikzpicture}
	\begin{axis}[
	    height=7cm,
	    width=14cm,
	    xmode=log,
	    cycle list name=exotic,
		xlabel=Base Learning Rate,
		ylabel=Accuracy
		%xmin=0,
		%xmax=30
		]
		\addlegendentry{16}
		\addplot  table [
	    	x=LR, y=16,  col sep=tab] {perp.dat};
		%\addlegendentry{32}
		%\addplot [orange] table [
	    	%x=LR, y=32, mark=none, col sep=tab] {perp.dat};
		\addlegendentry{64}
		\addplot  table [
	    	x=LR, y=64,  col sep=tab] {perp.dat};
		%\addlegendentry{128}
		%\addplot [green] table [
	    	%x=LR, y=128, mark=none, col sep=tab] {perp.dat};
		\addlegendentry{256}
		\addplot  table [
	    	x=LR, y=256, col sep=tab] {perp.dat};
		%\addlegendentry{512}
		%\addplot [indigo] table [
	    	%x=LR, y=512, mark=none, col sep=tab] {perp.dat};
		\addlegendentry{1024}
		\addplot  table [
	    	x=LR, y=1024,  col sep=tab] {perp.dat};
		%\addlegendentry{2048}
		%\addplot [orange] table [
	    	%x=LR, y=2048, mark=none, col sep=tab] {perp.dat};
		\addlegendentry{4096}
		\addplot  table [
	    	x=LR, y=4096,  col sep=tab] {perp.dat};
	\end{axis}
\end{tikzpicture} 
\caption{Accuracies achieved by hidden layer sizes for different learning rates}
\label{english-frequency}
\end{figure}

It is clear from the data that smaller hidden layer sizes performed
better with higher learning rates than did larger networks. One
explanation for this is that the smaller nets tend are simpler
(due to containing fewer hidden units); the smaller models are simpler
in the sense that there is less fine tuning of connection weights
required to effectively make use of the hidden units.
The smaller learning rates resulted in lower accuracies likely
because the simpler networks had not been trained enough.

With a larger hidden unit count, the connection weights need to be
very precise in order to balance out the connections amongst the
hidden units. When this precision is achieved with the lower
learning rates, the models are able to perform well on the test
data. An explanation to why this small amount of training was
enough for the larger networks but the smaller ones might be that
the shear number of neurons made it possible model the data with
little training. But in this case, the network would likely still 
benefit from a longer training time.


In the future, it would be important to include a validation set
in order to determine if the models are overfitting to the training
data as this risk increases as the number of hidden unit
gets larger.



\iffalse
1	16	0.3333
0.1	16	0.6781
0.01	16	0.7197
0.001	16	0.3724
0.0001	16	0.1574
1.00E-05	16	0.0957
1	32	0.1032
0.1	32	0.7126
0.01	32	0.854
0.001	32	0.441
0.0001	32	0.1577
1.00E-05	32	0.0684
1	64	0.0974
0.1	64	0.4855
0.01	64	0.8769
0.001	64	0.7568
0.0001	64	0.299
1.00E-05	64	0.1577
1	128	0.1032
0.1	128	0.0987
0.01	128	0.8799
0.001	128	0.8126
0.0001	128	0.4422
1.00E-05	128	0.1626
1	256	0.098
0.1	256	0.1009
0.01	256	0.8725
0.001	256	0.8362
0.0001	256	0.6067
1.00E-05	256	0.2174
1	512	0.1032
0.1	512	0.0974
0.01	512	0.6754
0.001	512	0.8654
0.0001	512	0.7034
1.00E-05	512	0.2867
1	1024	0.1028
0.1	1024	0.098
0.01	1024	0.0892
0.001	1024	0.864
0.0001	1024	0.7645
1.00E-05	1024	0.4777
1	2048	0.0892
0.1	2048	0.1009
0.01	2048	0.1135
0.001	2048	0.8056
0.0001	2048	0.8063
1.00E-05	2048	0.595
1	4096	0.1009
0.1	4096	0.0958
0.01	4096	0.1009
0.001	4096	0.7913
0.0001	4096	0.8105
1.00E-05	4096	0.6625
\fi


\end{document}
